{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM87AgBuWlBVSsUu/Kl/Nbz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wvsvenkat/MyoPS2020/blob/main/uNetTwoStage.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "id": "p3Dnxr2n0gqg",
        "outputId": "058834ef-ad30-4594-91ef-c93245b15052"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hMounted at /content/drive\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for UNet:\n\tsize mismatch for model.2.0.conv.weight: copying a param with shape torch.Size([32, 4, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 2, 3, 3, 3]).\n\tsize mismatch for model.2.0.conv.bias: copying a param with shape torch.Size([4]) from checkpoint, the shape in current model is torch.Size([2]).\n\tsize mismatch for model.2.1.conv.unit0.conv.weight: copying a param with shape torch.Size([4, 4, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([2, 2, 3, 3, 3]).\n\tsize mismatch for model.2.1.conv.unit0.conv.bias: copying a param with shape torch.Size([4]) from checkpoint, the shape in current model is torch.Size([2]).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3747822028.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0mckpt_stage1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/MyoPS2020/checkpoints/unet_background.pth\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0mstate1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_stage1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m \u001b[0mstage1_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_state\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0mstage1_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Stage 1 model loaded.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2623\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2624\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   2625\u001b[0m                 \"Error(s) in loading state_dict for {}:\\n\\t{}\".format(\n\u001b[1;32m   2626\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\\t\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for UNet:\n\tsize mismatch for model.2.0.conv.weight: copying a param with shape torch.Size([32, 4, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 2, 3, 3, 3]).\n\tsize mismatch for model.2.0.conv.bias: copying a param with shape torch.Size([4]) from checkpoint, the shape in current model is torch.Size([2]).\n\tsize mismatch for model.2.1.conv.unit0.conv.weight: copying a param with shape torch.Size([4, 4, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([2, 2, 3, 3, 3]).\n\tsize mismatch for model.2.1.conv.unit0.conv.bias: copying a param with shape torch.Size([4]) from checkpoint, the shape in current model is torch.Size([2])."
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# üì¶ Setup & Imports\n",
        "# ============================================\n",
        "!pip install monai nibabel pydicom pandas -q\n",
        "!pip install torch torchvision torchaudio -q\n",
        "\n",
        "import os, re, zipfile, glob\n",
        "import torch\n",
        "import numpy as np\n",
        "import nibabel as nib\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from monai.transforms import (\n",
        "    LoadImaged, EnsureChannelFirstd, ScaleIntensityd, EnsureTyped,\n",
        "    Resized, Compose, ConcatItemsd, Transform\n",
        ")\n",
        "from monai.data import Dataset\n",
        "from monai.metrics import DiceMetric\n",
        "from monai.losses import DiceCELoss\n",
        "from monai.networks.nets import UNet\n",
        "from google.colab import drive\n",
        "\n",
        "# ============================================\n",
        "# üìÅ Mount Google Drive\n",
        "# ============================================\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ============================================\n",
        "# üß¨ Data Preparation\n",
        "# ============================================\n",
        "def list_files(root):\n",
        "    exts = (\"*.nii\", \"*.nii.gz\")\n",
        "    files = []\n",
        "    for ext in exts:\n",
        "        files.extend(glob.glob(os.path.join(root, \"**\", ext), recursive=True))\n",
        "    return sorted(files)\n",
        "\n",
        "train_zip = \"/content/drive/MyDrive/MyoPS2020/train25.zip\"\n",
        "gt_zip    = \"/content/drive/MyDrive/MyoPS2020/train25_myops_gd.zip\"\n",
        "extract_path = \"/content/myops_mm3d\"\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "# Extract (idempotent per session)\n",
        "with zipfile.ZipFile(train_zip, 'r') as z:\n",
        "    z.extractall(extract_path + \"/train\")\n",
        "with zipfile.ZipFile(gt_zip, 'r') as z:\n",
        "    z.extractall(extract_path + \"/labels\")\n",
        "\n",
        "train_root = os.path.join(extract_path, \"train\")\n",
        "label_root = os.path.join(extract_path, \"labels\")\n",
        "if len(os.listdir(train_root)) == 1:\n",
        "    train_root = os.path.join(train_root, os.listdir(train_root)[0])\n",
        "if len(os.listdir(label_root)) == 1:\n",
        "    label_root = os.path.join(label_root, os.listdir(label_root)[0])\n",
        "\n",
        "train_files_all = list_files(train_root)\n",
        "label_files_all = list_files(label_root)\n",
        "\n",
        "def modality_of(path):\n",
        "    name = os.path.basename(path).lower()\n",
        "    if \"_c0\" in name: return \"bssfp\"\n",
        "    if \"_t2\" in name: return \"t2\"\n",
        "    if \"_de\" in name: return \"lge\"\n",
        "    return None\n",
        "\n",
        "def case_id_of(path):\n",
        "    name = os.path.basename(path).lower()\n",
        "    name = name.replace(\"_c0\", \"\").replace(\"_t2\", \"\").replace(\"_de\", \"\").replace(\"_gd\", \"\")\n",
        "    return re.sub(r\"\\.nii(\\.gz)?$\", \"\", name)\n",
        "\n",
        "cases = {}\n",
        "for f in train_files_all:\n",
        "    mod = modality_of(f)\n",
        "    cid = case_id_of(f)\n",
        "    if mod:\n",
        "        cases.setdefault(cid, {})[mod] = f\n",
        "\n",
        "labels_by_case = {case_id_of(f): f for f in label_files_all}\n",
        "\n",
        "items = []\n",
        "for cid, mods in cases.items():\n",
        "    if all(k in mods for k in [\"lge\", \"t2\", \"bssfp\"]) and cid in labels_by_case:\n",
        "        items.append({\n",
        "            \"lge\": mods[\"lge\"],\n",
        "            \"t2\": mods[\"t2\"],\n",
        "            \"bssfp\": mods[\"bssfp\"],\n",
        "            \"label\": labels_by_case[cid],\n",
        "            \"case_id\": cid\n",
        "        })\n",
        "\n",
        "# Label remapping (original codes -> 0..3)\n",
        "class FixedRemapLabels(Transform):\n",
        "    def __init__(self, mapping):\n",
        "        self.mapping = mapping\n",
        "    def __call__(self, data):\n",
        "        label = data[\"label\"]\n",
        "        if isinstance(label, np.ndarray):\n",
        "            label = torch.from_numpy(label)\n",
        "        # ensure dtype long\n",
        "        label = label.long()\n",
        "        for orig, target in self.mapping.items():\n",
        "            label[label == orig] = target\n",
        "        valid_targets = torch.tensor(list(self.mapping.values()), dtype=label.dtype)\n",
        "        mask = ~torch.isin(label, valid_targets)\n",
        "        label[mask] = 0\n",
        "        data[\"label\"] = label\n",
        "        return data\n",
        "\n",
        "label_mapping = {0: 0, 500: 1, 200: 2, 75: 3, 60: 3}\n",
        "target_size = (128, 128, 64)\n",
        "\n",
        "base_transforms = Compose([\n",
        "    LoadImaged(keys=[\"lge\", \"t2\", \"bssfp\", \"label\"]),\n",
        "    EnsureChannelFirstd(keys=[\"lge\", \"t2\", \"bssfp\", \"label\"]),\n",
        "    ScaleIntensityd(keys=[\"lge\", \"t2\", \"bssfp\"]),\n",
        "    FixedRemapLabels(label_mapping),\n",
        "    ConcatItemsd(keys=[\"lge\", \"t2\", \"bssfp\"], name=\"image\", dim=0),\n",
        "    Resized(keys=[\"image\", \"label\"], spatial_size=target_size),\n",
        "    EnsureTyped(keys=[\"image\", \"label\"])\n",
        "])\n",
        "\n",
        "train_items, val_items = train_test_split(items, test_size=0.2, random_state=42)\n",
        "train_ds = Dataset(data=train_items, transform=base_transforms)\n",
        "val_ds   = Dataset(data=val_items, transform=base_transforms)\n",
        "train_loader = DataLoader(train_ds, batch_size=1, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=1)\n",
        "\n",
        "# ============================================\n",
        "# üß† Stage 1: Load Pretrained Multi-class UNet (4 channels)\n",
        "# ============================================\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "stage1_model = UNet(\n",
        "    spatial_dims=3,\n",
        "    in_channels=3,\n",
        "    out_channels=4,  # bg + myocardium + infarction + edema\n",
        "    channels=(16, 32, 64, 128),\n",
        "    strides=(2, 2, 2),\n",
        "    num_res_units=2,\n",
        ").to(device)\n",
        "\n",
        "ckpt_stage1 = \"/content/drive/MyDrive/MyoPS2020/checkpoints/unet_background.pth\"\n",
        "state1 = torch.load(ckpt_stage1, map_location=device)\n",
        "stage1_model.load_state_dict(state1[\"model_state\"])\n",
        "stage1_model.eval()\n",
        "print(\"Stage 1 model loaded (4-class).\")\n",
        "\n",
        "# ============================================\n",
        "# üß† Stage 2: Foreground Segmenter (4 channels)\n",
        "# ============================================\n",
        "stage2_model = UNet(\n",
        "    spatial_dims=3,\n",
        "    in_channels=3,\n",
        "    out_channels=4,  # bg + myocardium + infarction + edema\n",
        "    channels=(16, 32, 64, 128),\n",
        "    strides=(2, 2, 2),\n",
        "    num_res_units=2,\n",
        ").to(device)\n",
        "\n",
        "loss_fn2 = DiceCELoss(\n",
        "    include_background=False,\n",
        "    to_onehot_y=True,\n",
        "    softmax=True,\n",
        "    lambda_dice=0.7,\n",
        "    lambda_ce=0.3\n",
        ")\n",
        "optimizer2 = torch.optim.Adam(stage2_model.parameters(), lr=1e-4)\n",
        "scheduler2 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer2, mode='min', patience=3, factor=0.5)\n",
        "dice_metric2 = DiceMetric(include_background=False, reduction=\"none\")\n",
        "\n",
        "# ============================================\n",
        "# üóÇÔ∏è Paths & Helpers\n",
        "# ============================================\n",
        "ckpt_dir = \"/content/drive/MyDrive/MyoPS2020/checkpoints\"\n",
        "pred_dir = \"/content/drive/MyDrive/MyoPS2020/predictions_stage2\"\n",
        "log_csv  = os.path.join(pred_dir, \"val_summary_stage2.csv\")\n",
        "os.makedirs(ckpt_dir, exist_ok=True)\n",
        "os.makedirs(pred_dir, exist_ok=True)\n",
        "\n",
        "class_names = [\"Myocardium\", \"Infarction\", \"Edema\"]\n",
        "val_interval = 5\n",
        "max_epochs = 50\n",
        "patience_limit = 5\n",
        "\n",
        "def save_checkpoint_stage2(epoch, best_loss, patience_counter, train_losses, val_history, path):\n",
        "    state = {\n",
        "        \"epoch\": epoch,\n",
        "        \"model_state\": stage2_model.state_dict(),\n",
        "        \"optimizer_state\": optimizer2.state_dict(),\n",
        "        \"scheduler_state\": scheduler2.state_dict(),\n",
        "        \"best_loss\": best_loss,\n",
        "        \"patience_counter\": patience_counter,\n",
        "        \"train_losses\": train_losses,\n",
        "        \"val_history\": val_history,\n",
        "    }\n",
        "    torch.save(state, path)\n",
        "    print(f\"Saved Stage 2 checkpoint: {path}\")\n",
        "\n",
        "# ============================================\n",
        "# üîÅ Stage 2 Training + Validation + NIfTI Saving\n",
        "# ============================================\n",
        "val_history2 = []\n",
        "train_losses2 = []\n",
        "best_loss = float(\"inf\")\n",
        "patience_counter = 0\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "    stage2_model.train()\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        images, labels = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
        "\n",
        "        # Stage 1 forward -> collapse to binary foreground mask\n",
        "        with torch.no_grad():\n",
        "            logits1 = stage1_model(images)                 # [B,4,H,W,D]\n",
        "            preds1 = torch.argmax(logits1, dim=1, keepdim=True)  # [B,1,H,W,D] class index\n",
        "            mask1  = (preds1 > 0).long()                   # foreground = any non-zero class\n",
        "\n",
        "        masked_images = images * mask1.float()\n",
        "\n",
        "        optimizer2.zero_grad()\n",
        "        outputs = stage2_model(masked_images)\n",
        "        loss = loss_fn2(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer2.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    avg_loss = epoch_loss / max(1, len(train_loader))\n",
        "    train_losses2.append(avg_loss)\n",
        "    print(f\"[Stage2] Epoch {epoch+1}/{max_epochs} - Train Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # --- Validation every val_interval epochs ---\n",
        "    if (epoch + 1) % val_interval == 0:\n",
        "        stage2_model.eval()\n",
        "        val_loss = 0.0\n",
        "        dice_rows = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                images, labels = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
        "                case_id = batch[\"case_id\"][0]\n",
        "\n",
        "                # Stage 1 mask\n",
        "                logits1 = stage1_model(images)\n",
        "                preds1 = torch.argmax(logits1, dim=1, keepdim=True)\n",
        "                mask1  = (preds1 > 0).long()\n",
        "                masked_images = images * mask1.float()\n",
        "\n",
        "                # Stage 2 forward\n",
        "                outputs = stage2_model(masked_images)\n",
        "                val_loss += loss_fn2(outputs, labels).item()\n",
        "\n",
        "                # Per-class Dice\n",
        "                dice_scores = dice_metric2(y_pred=outputs, y=labels).cpu().numpy().flatten()\n",
        "                dice_scores = np.nan_to_num(dice_scores, nan=0.0)\n",
        "                row = {name: float(dice_scores[i]) for i, name in enumerate(class_names)}\n",
        "                dice_rows.append(row)\n",
        "\n",
        "                # Save NIfTI prediction for visual inspection\n",
        "                pred = torch.argmax(outputs, dim=1)[0].cpu().numpy()  # [H,W,D]\n",
        "                affine = np.eye(4)\n",
        "                out_path = os.path.join(pred_dir, f\"{case_id}_stage2_pred_epoch{epoch+1}.nii.gz\")\n",
        "                nib.save(nib.Nifti1Image(pred.astype(np.uint8), affine), out_path)\n",
        "\n",
        "        val_loss /= max(1, len(val_loader))\n",
        "        mean_dice = float(np.mean([np.mean(list(r.values())) for r in dice_rows])) if dice_rows else 0.0\n",
        "        per_class_means = {\n",
        "            f\"dice_{k}\": float(np.mean([r[k] for r in dice_rows])) if dice_rows else 0.0\n",
        "            for k in class_names\n",
        "        }\n",
        "\n",
        "        print(f\"[Stage2] Epoch {epoch+1} - Val Loss: {val_loss:.4f}, \"\n",
        "              f\"Mean Dice: {mean_dice:.3f}, Myocardium: {per_class_means['dice_Myocardium']:.3f}\")\n",
        "\n",
        "        # Log summary\n",
        "        val_history2.append({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"val_loss\": val_loss,\n",
        "            \"mean_dice\": mean_dice,\n",
        "            **per_class_means\n",
        "        })\n",
        "\n",
        "        # Scheduler + early stopping on val loss\n",
        "        scheduler2.step(val_loss)\n",
        "        if val_loss < best_loss:\n",
        "            best_loss = val_loss\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience_limit:\n",
        "                print(\"Early stopping Stage 2.\")\n",
        "                # Save final checkpoint before break\n",
        "                save_checkpoint_stage2(epoch + 1, best_loss, patience_counter, train_losses2, val_history2,\n",
        "                                       os.path.join(ckpt_dir, f\"unet_stage2_final.pth\"))\n",
        "                break\n",
        "\n",
        "        # Save checkpoint at validation intervals\n",
        "        ckpt_path = os.path.join(ckpt_dir, f\"unet_stage2_epoch{epoch+1}.pth\")\n",
        "        save_checkpoint_stage2(epoch + 1, best_loss, patience_counter, train_losses2, val_history2, ckpt_path)\n",
        "\n",
        "# ============================================\n",
        "# üíæ Save terminal snapshot (if not early-stopped)\n",
        "# ============================================\n",
        "final_ckpt = os.path.join(ckpt_dir, \"unet_stage2_terminal.pth\")\n",
        "save_checkpoint_stage2(epoch + 1, best_loss, patience_counter, train_losses2, val_history2, final_ckpt)\n",
        "\n",
        "# ============================================\n",
        "# üìâ Plot Stage 2 Training Loss\n",
        "# ============================================\n",
        "plt.figure(figsize=(7,4))\n",
        "plt.plot(train_losses2, marker='o')\n",
        "plt.title(\"Stage 2 Training Loss Curve\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# ============================================\n",
        "# üìí Save Stage 2 Validation Summary CSV\n",
        "# ============================================\n",
        "pd.DataFrame(val_history2).to_csv(log_csv, index=False)\n",
        "print(f\"Saved Stage 2 validation summary to {log_csv}\")"
      ]
    }
  ]
}