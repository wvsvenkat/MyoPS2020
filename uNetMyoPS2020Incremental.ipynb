{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNPcuhR168OknX5+BtlyRGy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wvsvenkat/MyoPS2020/blob/main/uNetMyoPS2020Incremental.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSV9diUMc-gc",
        "outputId": "c3645522-a48b-48a2-c41d-745bc3f11a59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Do you want to resume training from last checkpoint? (y/n): y\n",
            "Resuming from /content/drive/MyDrive/MyoPS2020/checkpoints/unet_epoch25.pth\n",
            "Epoch 26/50 - Train Loss: 0.6641\n",
            "Epoch 27/50 - Train Loss: 0.6542\n",
            "Epoch 28/50 - Train Loss: 0.6414\n",
            "Epoch 29/50 - Train Loss: 0.6280\n",
            "Epoch 30/50 - Train Loss: 0.6175\n",
            "Saved checkpoint: /content/drive/MyDrive/MyoPS2020/checkpoints/unet_epoch30.pth\n",
            "Epoch 30 - Val Loss: 0.6345, Mean Myocardium Dice: 0.017\n",
            "Epoch 31/50 - Train Loss: 0.6053\n",
            "Epoch 32/50 - Train Loss: 0.5915\n",
            "Epoch 33/50 - Train Loss: 0.5807\n",
            "Epoch 34/50 - Train Loss: 0.5684\n",
            "Epoch 35/50 - Train Loss: 0.5595\n",
            "Saved checkpoint: /content/drive/MyDrive/MyoPS2020/checkpoints/unet_epoch35.pth\n",
            "Epoch 35 - Val Loss: 0.5818, Mean Myocardium Dice: 0.017\n",
            "Epoch 36/50 - Train Loss: 0.5471\n",
            "Epoch 37/50 - Train Loss: 0.5349\n",
            "Epoch 38/50 - Train Loss: 0.5240\n",
            "Epoch 39/50 - Train Loss: 0.5132\n",
            "Epoch 40/50 - Train Loss: 0.5073\n",
            "Saved checkpoint: /content/drive/MyDrive/MyoPS2020/checkpoints/unet_epoch40.pth\n",
            "Epoch 40 - Val Loss: 0.5515, Mean Myocardium Dice: 0.017\n",
            "Epoch 41/50 - Train Loss: 0.4966\n",
            "Epoch 42/50 - Train Loss: 0.4866\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# üì¶ Setup & Imports\n",
        "# ============================================\n",
        "!pip install monai nibabel pydicom pandas -q\n",
        "!pip install torch torchvision torchaudio -q\n",
        "\n",
        "import os, re, zipfile, glob\n",
        "import torch\n",
        "import numpy as np\n",
        "import nibabel as nib\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from monai.networks import one_hot\n",
        "\n",
        "from monai.transforms import (\n",
        "    LoadImaged, EnsureChannelFirstd, ScaleIntensityd, EnsureTyped,\n",
        "    Resized, Compose, ConcatItemsd, Transform\n",
        ")\n",
        "from monai.data import Dataset\n",
        "from monai.metrics import DiceMetric\n",
        "from monai.losses import DiceLoss\n",
        "from monai.networks.nets import UNet\n",
        "from google.colab import drive\n",
        "from monai.losses import DiceCELoss\n",
        "\n",
        "import torch\n",
        "\n",
        "class WeightedDiceCELoss(torch.nn.Module):\n",
        "    def __init__(self, dice_ce_loss, class_weights):\n",
        "        super().__init__()\n",
        "        self.dice_ce_loss = dice_ce_loss\n",
        "        self.class_weights = torch.tensor(class_weights).float()\n",
        "\n",
        "    def forward(self, outputs, labels):\n",
        "        # MONAI handles one-hot internally, so labels should be [B, 1, H, W, D]\n",
        "        loss = self.dice_ce_loss(outputs, labels)  # returns [B, C]\n",
        "        if loss.ndim == 2 and loss.shape[1] == len(self.class_weights):\n",
        "            weights = self.class_weights.to(outputs.device)\n",
        "            weighted = loss * weights.unsqueeze(0)  # broadcast over batch\n",
        "            return weighted.mean()\n",
        "        else:\n",
        "            return loss.mean()  # fallback if shape unexpected\n",
        "\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# üìÅ Mount Google Drive & Extract Data\n",
        "# ============================================\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "train_zip = \"/content/drive/MyDrive/MyoPS2020/train25.zip\"\n",
        "gt_zip    = \"/content/drive/MyDrive/MyoPS2020/train25_myops_gd.zip\"\n",
        "\n",
        "extract_path = \"/content/myops_mm3d\"\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(train_zip, 'r') as z:\n",
        "    z.extractall(extract_path + \"/train\")\n",
        "with zipfile.ZipFile(gt_zip, 'r') as z:\n",
        "    z.extractall(extract_path + \"/labels\")\n",
        "\n",
        "train_root = os.path.join(extract_path, \"train\")\n",
        "label_root = os.path.join(extract_path, \"labels\")\n",
        "if len(os.listdir(train_root)) == 1:\n",
        "    train_root = os.path.join(train_root, os.listdir(train_root)[0])\n",
        "if len(os.listdir(label_root)) == 1:\n",
        "    label_root = os.path.join(label_root, os.listdir(label_root)[0])\n",
        "\n",
        "# ============================================\n",
        "# üß¨ Group Files by Case & Modality\n",
        "# ============================================\n",
        "def list_files(root):\n",
        "    exts = (\"*.nii\", \"*.nii.gz\")\n",
        "    files = []\n",
        "    for ext in exts:\n",
        "        files.extend(glob.glob(os.path.join(root, \"**\", ext), recursive=True))\n",
        "    return sorted(files)\n",
        "\n",
        "train_files_all = list_files(train_root)\n",
        "label_files_all = list_files(label_root)\n",
        "\n",
        "def modality_of(path):\n",
        "    name = os.path.basename(path).lower()\n",
        "    if \"_c0\" in name: return \"bssfp\"\n",
        "    if \"_t2\" in name: return \"t2\"\n",
        "    if \"_de\" in name: return \"lge\"\n",
        "    return None\n",
        "\n",
        "def case_id_of(path):\n",
        "    name = os.path.basename(path).lower()\n",
        "    name = name.replace(\"_c0\", \"\").replace(\"_t2\", \"\").replace(\"_de\", \"\").replace(\"_gd\", \"\")\n",
        "    return re.sub(r\"\\.nii(\\.gz)?$\", \"\", name)\n",
        "\n",
        "cases = {}\n",
        "for f in train_files_all:\n",
        "    mod = modality_of(f)\n",
        "    cid = case_id_of(f)\n",
        "    if mod:\n",
        "        cases.setdefault(cid, {})[mod] = f\n",
        "\n",
        "labels_by_case = {case_id_of(f): f for f in label_files_all}\n",
        "\n",
        "items = []\n",
        "for cid, mods in cases.items():\n",
        "    if all(k in mods for k in [\"lge\", \"t2\", \"bssfp\"]) and cid in labels_by_case:\n",
        "        items.append({\n",
        "            \"lge\": mods[\"lge\"],\n",
        "            \"t2\": mods[\"t2\"],\n",
        "            \"bssfp\": mods[\"bssfp\"],\n",
        "            \"label\": labels_by_case[cid],\n",
        "            \"case_id\": cid\n",
        "        })\n",
        "\n",
        "# ============================================\n",
        "# üßº Fixed Label Remapping Transform\n",
        "# ============================================\n",
        "class FixedRemapLabels(Transform):\n",
        "    def __init__(self, mapping):\n",
        "        self.mapping = mapping\n",
        "\n",
        "    def __call__(self, data):\n",
        "        label = data[\"label\"]\n",
        "        # Expect label as torch.Tensor after EnsureTyped; handle numpy too\n",
        "        if isinstance(label, np.ndarray):\n",
        "            label = torch.from_numpy(label)\n",
        "        for orig, target in self.mapping.items():\n",
        "            label[label == orig] = target\n",
        "        valid_targets = torch.tensor(list(self.mapping.values()), dtype=label.dtype)\n",
        "        mask = ~torch.isin(label, valid_targets)\n",
        "        label[mask] = 0\n",
        "        data[\"label\"] = label\n",
        "        return data\n",
        "\n",
        "label_mapping = {0: 0, 500: 1, 200: 2, 75: 3, 60: 3}\n",
        "\n",
        "# ============================================\n",
        "# üîÑ Transforms, Dataset, DataLoader\n",
        "# ============================================\n",
        "target_size = (128, 128, 64)\n",
        "\n",
        "base_transforms = Compose([\n",
        "    LoadImaged(keys=[\"lge\", \"t2\", \"bssfp\", \"label\"]),\n",
        "    EnsureChannelFirstd(keys=[\"lge\", \"t2\", \"bssfp\", \"label\"]),\n",
        "    ScaleIntensityd(keys=[\"lge\", \"t2\", \"bssfp\"]),\n",
        "    FixedRemapLabels(label_mapping),\n",
        "    ConcatItemsd(keys=[\"lge\", \"t2\", \"bssfp\"], name=\"image\", dim=0),\n",
        "    Resized(keys=[\"image\", \"label\"], spatial_size=target_size),\n",
        "    EnsureTyped(keys=[\"image\", \"label\"])\n",
        "])\n",
        "\n",
        "train_items, val_items = train_test_split(items, test_size=0.2, random_state=42)\n",
        "train_ds = Dataset(data=train_items, transform=base_transforms)\n",
        "val_ds   = Dataset(data=val_items, transform=base_transforms)\n",
        "train_loader = DataLoader(train_ds, batch_size=1, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=1)\n",
        "\n",
        "# ============================================\n",
        "# üß† Model, Loss, Optimizer, Scheduler\n",
        "# ============================================\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = UNet(\n",
        "    spatial_dims=3,\n",
        "    in_channels=3,\n",
        "    out_channels=4,  # background + 3 classes -> use softmax + to_onehot_y\n",
        "    channels=(16, 32, 64, 128),\n",
        "    strides=(2, 2, 2),\n",
        "    num_res_units=2,\n",
        ").to(device)\n",
        "\n",
        "# Dice + CE hybrid loss\n",
        "\n",
        "\n",
        "# Hybrid Dice + CE loss\n",
        "# Example: weight myocardium higher\n",
        "\n",
        "\n",
        "base_loss = DiceCELoss(\n",
        "    include_background=False,\n",
        "    to_onehot_y=True,\n",
        "    softmax=True,\n",
        "    lambda_dice=0.7,\n",
        "    lambda_ce=0.3\n",
        ")\n",
        "loss_fn = WeightedDiceCELoss(base_loss, class_weights=[2.0, 1.0, 1.0])  # [Myocardium, Infarction, Edema]\n",
        "#from monai.losses import BoundaryLoss\n",
        "#from monai.networks import one_hot\n",
        "\n",
        "#boundary_loss = BoundaryLoss(to_onehot_y=True, softmax=True)\n",
        "\n",
        "#def hybrid_loss(outputs, labels):\n",
        "#    dice_ce = loss_fn(outputs, labels)\n",
        "#    b_loss = boundary_loss(outputs, labels)\n",
        "#    return dice_ce + 0.1 * b_loss   # small weight for boundary term\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='min', patience=3, factor=0.5\n",
        ")\n",
        "dice_metric = DiceMetric(include_background=False, reduction=\"none\")\n",
        "\n",
        "# ============================================\n",
        "# üíæ Checkpoint helpers (save/load full state)\n",
        "# ============================================\n",
        "os.makedirs(\"/content/drive/MyDrive/MyoPS2020/checkpoints\", exist_ok=True)\n",
        "\n",
        "def latest_checkpoint(pattern=\"/content/drive/MyDrive/MyoPS2020/checkpoints/unet_epoch*.pth\"):\n",
        "    ckpts = sorted(glob.glob(pattern), key=lambda p: int(re.search(r\"epoch(\\d+)\", p).group(1)) if re.search(r\"epoch(\\d+)\", p) else -1)\n",
        "    return ckpts[-1] if ckpts else None\n",
        "\n",
        "def save_checkpoint(epoch, best_loss, patience_counter, train_losses, path):\n",
        "    state = {\n",
        "        \"epoch\": epoch,\n",
        "        \"model_state\": model.state_dict(),\n",
        "        \"optimizer_state\": optimizer.state_dict(),\n",
        "        \"scheduler_state\": scheduler.state_dict(),\n",
        "        \"best_loss\": best_loss,\n",
        "        \"patience_counter\": patience_counter,\n",
        "        \"train_losses\": train_losses,\n",
        "    }\n",
        "    torch.save(state, path)\n",
        "    print(f\"Saved checkpoint: {path}\")\n",
        "\n",
        "def load_checkpoint(path):\n",
        "    state = torch.load(path, map_location=device)\n",
        "    model.load_state_dict(state[\"model_state\"])\n",
        "    optimizer.load_state_dict(state[\"optimizer_state\"])\n",
        "    scheduler.load_state_dict(state[\"scheduler_state\"])\n",
        "    return state\n",
        "\n",
        "# ============================================\n",
        "# üîÅ Training with resume + validation/logging every 5 epochs\n",
        "# ============================================\n",
        "max_epochs = 50\n",
        "train_losses = []\n",
        "best_loss = float(\"inf\")\n",
        "patience_counter = 0\n",
        "patience_limit = 5\n",
        "val_interval = 5  # validate/save every 5 epochs\n",
        "\n",
        "# --- Resume or restart ---\n",
        "resume = input(\"Do you want to resume training from last checkpoint? (y/n): \").strip().lower()\n",
        "start_epoch = 0\n",
        "if resume == \"y\":\n",
        "    last_ckpt = latest_checkpoint()\n",
        "    if last_ckpt:\n",
        "        print(f\"Resuming from {last_ckpt}\")\n",
        "        state = load_checkpoint(last_ckpt)\n",
        "        start_epoch = state[\"epoch\"]\n",
        "        best_loss = state[\"best_loss\"]\n",
        "        patience_counter = state[\"patience_counter\"]\n",
        "        train_losses = state[\"train_losses\"]\n",
        "    else:\n",
        "        print(\"No checkpoint found, starting fresh.\")\n",
        "else:\n",
        "    print(\"Starting fresh.\")\n",
        "\n",
        "class_names = [\"Myocardium\", \"Infarction\", \"Edema\"]\n",
        "val_history_rows = []  # accumulates per-epoch (every 5) mean dice and val loss\n",
        "\n",
        "for epoch in range(start_epoch, max_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "    for batch in train_loader:\n",
        "        images, labels = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    avg_loss = epoch_loss / len(train_loader)\n",
        "    train_losses.append(avg_loss)\n",
        "    print(f\"Epoch {epoch+1}/{max_epochs} - Train Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # --- Save checkpoint every val_interval epochs ---\n",
        "    if (epoch + 1) % val_interval == 0:\n",
        "        ckpt_path = f\"/content/drive/MyDrive/MyoPS2020/checkpoints/unet_epoch{epoch+1}.pth\"\n",
        "        save_checkpoint(epoch + 1, best_loss, patience_counter, train_losses, ckpt_path)\n",
        "\n",
        "    # --- Validation every val_interval epochs ---\n",
        "    if (epoch + 1) % val_interval == 0:\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        dice_rows = []\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                images, labels = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
        "                outputs = model(images)\n",
        "                val_loss += loss_fn(outputs, labels).item()\n",
        "                dice_scores = dice_metric(y_pred=outputs, y=labels).cpu().numpy().flatten()\n",
        "                dice_scores = np.nan_to_num(dice_scores, nan=0.0)  # guard against NaN\n",
        "                row = {name: float(dice_scores[i]) for i, name in enumerate(class_names)}\n",
        "                dice_rows.append(row)\n",
        "\n",
        "        val_loss /= max(1, len(val_loader))  # avoid division by zero\n",
        "        if dice_rows:\n",
        "            mean_dice = float(np.mean([np.mean(list(r.values())) for r in dice_rows]))\n",
        "        else:\n",
        "            mean_dice = 0.0\n",
        "\n",
        "        # ‚úÖ Always print, even if mean_dice is 0\n",
        "        myo_dice = float(np.mean([r[\"Myocardium\"] for r in dice_rows])) if dice_rows else 0.0\n",
        "        print(f\"Epoch {epoch+1} - Val Loss: {val_loss:.4f}, Mean Myocardium Dice: {myo_dice:.3f}\")\n",
        "\n",
        "        # Log safely\n",
        "        val_history_rows.append({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"val_loss\": val_loss,\n",
        "            \"mean_dice\": mean_dice,\n",
        "            **{f\"dice_{name}\": float(np.mean([r[name] for r in dice_rows])) if dice_rows else 0.0\n",
        "              for name in class_names}\n",
        "        })\n",
        "\n",
        "        # LR schedule on validation loss\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # Early stopping logic on val loss\n",
        "        if val_loss < best_loss:\n",
        "            best_loss = val_loss\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience_limit:\n",
        "                print(\"Early stopping triggered.\")\n",
        "                break\n",
        "\n",
        "# ============================================\n",
        "# üíæ Save Final Model (terminal snapshot)\n",
        "# ============================================\n",
        "final_ckpt = \"/content/drive/MyDrive/MyoPS2020/checkpoints/unet_final.pth\"\n",
        "save_checkpoint(epoch + 1, best_loss, patience_counter, train_losses, final_ckpt)\n",
        "\n",
        "# ============================================\n",
        "# üìâ Plot Training Loss Curve\n",
        "# ============================================\n",
        "plt.figure(figsize=(7,4))\n",
        "plt.plot(train_losses, marker='o')\n",
        "plt.title(\"Training Loss Curve\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Dice Loss\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# ============================================\n",
        "# üìí Save validation summary CSV (every 5 epochs)\n",
        "# ============================================\n",
        "os.makedirs(\"/content/drive/MyDrive/MyoPS2020/predictions\", exist_ok=True)\n",
        "val_summary_path = \"/content/drive/MyDrive/MyoPS2020/predictions/val_summary.csv\"\n",
        "pd.DataFrame(val_history_rows).to_csv(val_summary_path, index=False)\n",
        "print(f\"Saved validation summary to {val_summary_path}\")\n",
        "\n",
        "# ============================================\n",
        "# üß™ Final validation pass: Dice logging + visualization + NIfTI export\n",
        "# ============================================\n",
        "model.eval()\n",
        "dice_rows = []\n",
        "class_names = [\"Myocardium\", \"Infarction\", \"Edema\"]\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "        images = batch[\"image\"].to(device)\n",
        "        labels = batch[\"label\"].to(device)\n",
        "        case_id = batch[\"case_id\"][0]\n",
        "        outputs = model(images)\n",
        "\n",
        "       # Dice per class for this case\n",
        "        dice_scores = dice_metric(y_pred=outputs, y=labels).cpu().numpy().flatten()\n",
        "        dice_scores = np.nan_to_num(dice_scores, nan=0.0)  # replace NaN with 0\n",
        "\n",
        "        dice_row = {\n",
        "            \"case_id\": case_id,\n",
        "            \"mean_dice\": float(np.mean(dice_scores)) if len(dice_scores) > 0 else 0.0\n",
        "        }\n",
        "\n",
        "        # Log per-class Dice safely\n",
        "        for i, name in enumerate(class_names):\n",
        "            score = float(dice_scores[i]) if i < len(dice_scores) else 0.0\n",
        "            dice_row[f\"dice_{name}\"] = score\n",
        "\n",
        "        # Optional: print myocardium Dice for quick feedback\n",
        "        if \"Myocardium\" in class_names:\n",
        "            print(f\"Case {case_id} - Myocardium Dice: {dice_row['dice_Myocardium']:.3f}\")\n",
        "\n",
        "        dice_rows.append(dice_row)\n",
        "\n",
        "        # Save prediction as NIfTI (argmax over classes)\n",
        "        pred = torch.argmax(outputs, dim=1)[0].cpu().numpy()\n",
        "        affine = np.eye(4)\n",
        "        nib.save(nib.Nifti1Image(pred.astype(np.uint8), affine), f\"predictions/{case_id}_pred.nii.gz\")\n",
        "\n",
        "        # Visualization: multiple slices\n",
        "        img_np = images[0].cpu().numpy()  # shape [C, H, W, D]\n",
        "        lbl_np = labels[0,0].cpu().numpy()  # single channel label [H, W, D]\n",
        "        slice_indices = [img_np.shape[-1]//4, img_np.shape[-1]//2, 3*img_np.shape[-1]//4]\n",
        "        for z in slice_indices:\n",
        "            plt.figure(figsize=(16,4))\n",
        "            plt.subplot(1,4,1); plt.imshow(img_np[0, ..., z], cmap=\"gray\"); plt.title(f\"C0 slice {z}\")\n",
        "            plt.subplot(1,4,2); plt.imshow(lbl_np[..., z], cmap=\"jet\", vmin=0, vmax=3); plt.title(\"Ground Truth\")\n",
        "            plt.subplot(1,4,3); plt.imshow(pred[..., z], cmap=\"jet\", vmin=0, vmax=3); plt.title(\"Prediction\")\n",
        "            match_mask = (lbl_np[..., z] == pred[..., z]).astype(np.uint8) * 255\n",
        "            plt.subplot(1,4,4); plt.imshow(match_mask, cmap=\"gray\"); plt.title(\"Match Mask\")\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "# Save per-case Dice scores to CSV\n",
        "df = pd.DataFrame(dice_rows)\n",
        "df[\"MeanDice\"] = df[class_names].mean(axis=1, skipna=True)\n",
        "dice_path = \"/content/drive/MyDrive/MyoPS2020/predictions/dice_scores.csv\"\n",
        "df.to_csv(dice_path, index=False)\n",
        "print(f\"Saved Dice scores to {dice_path}\")"
      ]
    }
  ]
}